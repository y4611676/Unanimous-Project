{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 影像分割 Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割（Segmentation）是指在影像處理中，將影像劃分為多個不同的區域或區塊的過程。這些區域可以是具有相似特徵的像素集合，例如顏色、亮度、紋理等。分割技術的目標是將影像中的每個像素分配到相應的區域，從而使得影像中的不同物體或結構可以被區分開來。\n",
    "\n",
    "在影像分割中，常見的方法包括：\n",
    "\n",
    "1. **基於閾值的分割**：根據像素的灰度值或顏色值，將影像劃分為前景和背景兩個區域。\n",
    "  \n",
    "2. **區域生長**：從種子點開始，根據像素的相似性，將相鄰的像素逐漸合併成為同一區域。\n",
    "  \n",
    "3. **邊緣檢測**：檢測影像中的邊緣，從而將影像劃分為不同的物體或結構。\n",
    "\n",
    "分割技術在計算機視覺和影像處理中具有廣泛的應用，包括但不限於：\n",
    "\n",
    "1. **醫學影像分析**：如MRI、CT影像中的器官和組織分割，用於疾病診斷和治療規劃。\n",
    "\n",
    "2. **地理信息系統（GIS）**：將衛星影像或地圖影像中的不同地物區分開來，用於土地利用分析、城市規劃等。\n",
    "\n",
    "3. **自動駕駛**：將道路影像中的行車線、車輛和行人等分割開來，用於自駕車的路徑規劃和障礙物檢測。\n",
    "\n",
    "總之，分割是影像處理中將影像劃分為多個區域的過程，其應用涉及多個領域，並在計算機視覺和圖像分析中扮演著重要角色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown '1y_zwNgVOxwxSus-CvQRzKsR5PJ-1p9nz' --output lesson9.zip\n",
    "!unzip lesson9.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- 如果您想在自己的電腦上執行此程式碼，您可以安裝以下內容:\n",
    "\n",
    "```\n",
    "    !pip install transformers\n",
    "    !pip install gradio\n",
    "    !pip install timm\n",
    "    !pip install torchvision\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U gradio timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 這是一些抑制警告訊息的程式碼。\n",
    "\n",
    "這段程式碼是從`transformers`庫中引入`logging`模組，然後設置日誌輸出的細節程度為錯誤級別（error）。這樣做的目的通常是為了避免顯示過多的日誌訊息，只保留錯誤和更高級別的訊息，以便更容易追蹤和處理錯誤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 SAM 產生遮罩 mask-generation\n",
    "\n",
    "[Segment Anything Model (SAM)](https://segment-anything.com) 模型由 Meta AI 發布。\n",
    "\n",
    "關於資訊 [Zigeng/SlimSAM-uniform-77](https://huggingface.co/Zigeng/SlimSAM-uniform-77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tramsformers import pipeline\n",
    "sam_pips = pipeline('mask-generation', 'Zigeng/SlimSAM-uniform-77')\n",
    "\n",
    "from PIL import Image\n",
    "raw_iamge = Image.open('/content/meta_liamas.jpg')\n",
    "raw_image.resize(720, 375)\n",
    "\n",
    "output = sam_pipe(raw_iamge, points_per_batch = 16)\n",
    "\n",
    "from helper import show_pipe_masks_on_image\n",
    "show_pipe_masks_on_iamge(raw_iamge, output)\n",
    "\n",
    "len(output['masks'])\n",
    "output['scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 運行這將需要一些時間\n",
    "-  `points_per_batch` 的值越高，管道推理就越高效\n",
    "_注意：_ 運行此程式碼時獲得的分段顏色可能與您在影片中看到的顏色不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更快的推理：推理影像和單點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamProcessor\n",
    "model = SmaModel.from_pretrained('Zigeng/SlimSAM-uniform-77')\n",
    "processor =  SmaProcessor.from_pretrained('Zigeng/SlimSAM-uniform-77')\n",
    "model\n",
    "                                          \n",
    "raw_iamge.resize((720, 375))\n",
    "input_points = [[[400, 200]]]\n",
    "inputs = processor(raw_image, input_points = input_points, return_tensors = 'pt')\n",
    "\n",
    "import torch\n",
    "with torch.no_grad():  outputs = model(''inputs)\n",
    "outputs\n",
    "\n",
    "predicted_masks = processor.image_processor.post_process_masks(\n",
    "    outputs.pred_masks,\n",
    "    inputs[\"original_sizes\"],\n",
    "    inputs[\"reshaped_input_sizes\"]\n",
    ")\n",
    "len(predicted_masks)\n",
    "\n",
    "predicted_mask = predicted_masks[0]\n",
    "predicted_mask.shape\n",
    "outputs.iou_scores\n",
    "\n",
    "from helper import show_mask_on_image\n",
    "for i in range(3):\n",
    "    show_mask_on_image(raw_image, predicted_mask[:, i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分割安德魯穿的藍色襯衫。\n",
    "- 給出該區域中的任何單一 2D 點（藍色襯衫）。\n",
    "- 使用影像和單點建立輸入。\n",
    "- `return_tensors=\"pt\"` 表示傳回 PyTorch 張量。\n",
    "- 給定輸入，從模型中取得輸出。\n",
    "`predicted_masks`的長度對應於輸入中使用的影像數量。\n",
    "- 檢查第一個 ([0]) 預測遮罩的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 DPT 進行深度估計 depth-estimation\n",
    "\n",
    "- Ranftl 等人在論文 [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) 中介紹了此模型。 (2021) 並首次在 [isl-org/DPT](https://github.com/isl-org/DPT) 中發布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_estimator = pipeline('depth-estimation','Intel/dpt-hybrid-midas')\n",
    "\n",
    "raw_image = Image.open('/content/gradio_tamagochi_vienna.png')\n",
    "raw_image.resize((806, 621))\n",
    "\n",
    "output = depth_estimator(raw_image)\n",
    "output\n",
    "\n",
    "output[\"predicted_depth\"].shape\n",
    "\n",
    "output[\"predicted_depth\"].unsqueeze(1).shape\n",
    "\n",
    "# resize the prediction 調整預測大小\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "    output[\"predicted_depth\"].unsqueeze(1),\n",
    "    size=raw_image.size[::-1],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ")\n",
    "prediction.shape\n",
    "\n",
    "raw_image.size[::-1]\n",
    "\n",
    "prediction\n",
    "\n",
    "import numpy as np\n",
    "# normalize the prediction 標準化預測\n",
    "output = prediction.squeeze().numpy() #降1個維度且轉成numpy數組\n",
    "formatted = (output * 255 / np.max(output)).astype(\"uint8\") #對深度值進行歸一化處理\n",
    "depth = Image.fromarray(formatted) #深度轉換為PIL圖像\n",
    "depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "關於資訊 ['Intel/dpt-hybrid-midas'](https://huggingface.co/Intel/dpt-hybrid-midas)\n",
    "- - 如果您想產生此圖片或類似的圖像，請查看 [Gradio](https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio/) 上的短期課程 並轉到“圖像生成應用程式 Image Generation App”課程。\n",
    "- 對輸出影像進行後處理，將其大小調整為原始影像的大小。\n",
    "- 將預測張量（0 到 255 之間）進行歸一化，以便可以顯示它們。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這段程式碼對模型預測的深度圖進行了後續處理和轉換成可視化的影像。以下是對程式碼的註解：\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# prediction是經過插值後的深度圖tensor，使用squeeze函數將其壓縮維度，將1維的channel維度去除，得到二維的深度圖tensor\n",
    "# 使用numpy()方法將tensor轉換為NumPy陣列\n",
    "output = prediction.squeeze().numpy()\n",
    "\n",
    "# 對深度值進行歸一化處理，將深度值映射到0到255之間\n",
    "# 將深度值乘以255除以最大深度值，然後轉換為整數型別（uint8）\n",
    "# 這樣處理後深度圖的深度值範圍就變成了0到255之間，便於可視化顯示\n",
    "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "\n",
    "# 將處理過的深度圖轉換為PIL圖像對象，以便後續的顯示或保存\n",
    "depth = Image.fromarray(formatted)\n",
    "\n",
    "```\n",
    "\n",
    "總的來說，這段程式碼將模型預測的深度圖進行了後續的處理，使其變得更容易理解和顯示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Gradio 進行演示\n",
    "\n",
    "### 故障排除提示\n",
    "- 請注意，在課堂上，您可能會看到用於建立 Gradio 應用程式的程式碼無限期地運行。\n",
    "   - 這是特定於該課堂環境的，當它同時為許多學習者提供服務時，如果您在自己的電腦上執行此程式碼，您將不會遇到此問題。\n",
    "- 若要解決此問題，請重新啟動核心（選單「核心」->「重新啟動核心」）並從課程開始重新執行實驗室中的程式碼。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更快的推理：推理影像和單點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "def launch(input_image):\n",
    "  out = depth_estimator(input_image)\n",
    "  # resize the prediction調整預測大小\n",
    "  prediction = torch.nn.functional.interpolate(\n",
    "    out[\"predicted_depth\"].unsqueeze(1),\n",
    "    size=input_image.size[::-1],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,)\n",
    "\n",
    "  # normalize the prediction 標準化預測\n",
    "  output = prediction.squeeze().numpy() #降1個維度且轉成numpy數組\n",
    "  formatted = (output * 255 / np.max(output)).astype(\"uint8\") #對深度值進行歸一化處理\n",
    "  depth = Image.fromarray(formatted) #深度轉換為PIL圖像\n",
    "  return depth\n",
    "\n",
    "iface = gr.Interface(launch, gr.Image(type=\"pil\"), gr.Image(type=\"pil\"))\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
